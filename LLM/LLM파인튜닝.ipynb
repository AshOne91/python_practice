{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac93914",
   "metadata": {},
   "source": [
    "파인튜닝\n",
    "```\n",
    "LLM을 추가 학습\n",
    "소량의 고품질데이터로 모델을 재조정해서 작업별 성능 향상\n",
    "```\n",
    "파인튜닝VS프롬프트엔지니어링\n",
    "```\n",
    "프롬프트엔지니어링\n",
    "    모델의 가중치를 변경하지않고 입력프롬프트를 조정해 원하는 출력을 유도\n",
    "```\n",
    "파인튜닝\n",
    "```\n",
    "    모델의 가중치를 업데이트 특정 데이터셋에 최적화\n",
    "    종류:\n",
    "        Full File-Tuning : 모델의 모든 가중치를 튜닝\n",
    "        PEFT(Parameter-Effective Fine-Tuning) : 소수의 파라메터만 업데이트 효율성 극대화\n",
    "            LoRA, QLoRA\n",
    "```\n",
    "LoRA(Low-Rank Adaptation), QLoRA(Quantized LoRA)\n",
    "```\n",
    "    LoRA : 모델의 가중치 행렬에 소규모 추가 파라메터(저랭크 행렬)를 삽입하여 학습\n",
    "    원본모델은유지, 어뎁터만 저장/배포\n",
    "    Llama-3를 한국어 데이터로 LoRA파인튜닝\n",
    "\n",
    "    QLoRA : LoRA에 4비트 또는 8비트 양자를 적용 메모리사용량 절감\n",
    "```\n",
    "한국어 LLM파인튜닝\n",
    "```\n",
    "    영어중심의 LLM 한국어 데이터가 부족\n",
    "    토크나이저가 한국어에 최적화 안됨\n",
    "    해결 : 한국어 특화모델(KoGPT2, Llama-3-Open-Ko, Mistal-Ko)\n",
    "        한국어 데이터셋 이용(KoAlpaca, NIA한국어 대회 데이터셋)\n",
    "        토크나이저 재학습, 한국어 전용 토크나이져\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33f1de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForCausalLM' from 'transformers' (c:\\Users\\USER\\miniconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#  Lima-3 --> 경량화 모델\u001b[39;00m\n\u001b[32m      3\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mfacebook/opt-125m\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AutoModelForCausalLM' from 'transformers' (c:\\Users\\USER\\miniconda3\\envs\\LLM\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#  Lima-3 --> 경량화 모델\n",
    "model_name = \"facebook/opt-125m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb63c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install peft\n",
    "# Lora 설정 정의\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
